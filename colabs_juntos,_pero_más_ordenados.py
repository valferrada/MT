# -*- coding: utf-8 -*-
"""Colabs juntos, pero más ordenados.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rsPqPgLqil1UWqaoIWUqVgBc-AK592hC

# **APERTURA DE DOCUMENTO**
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/DataCORFO/Data_Sercotec.txt', delimiter="\t" , error_bad_lines=False)

columnas_a_dejar=['postulante_id','pregunta_id','respuesta_id', 'texto.respuesta', 'nota']

df = df.rename(columns={'postulante_beneficiario.id_beneficiario': 'postulante_id'})

df['texto.respuesta'] = df['texto.respuesta'].str.lower() #MINUSCULAS

print(df.columns)
df=df[columnas_a_dejar]

"""**AED**"""

print('Cantidad de Filas y columnas:', df.shape)
print('Nombre columnas:', df.columns)

df.info()

df['nota'].describe()

df.head(22000)

df.tail(20000)

#creacion datas separados x seccion de canvas
df_1 = df
df_2 = df
df_3 = df
df_4 = df
df_5 = df
df_6 = df
df_7 = df
df_8 = df
df_9 = df

#limpieza de respuestas distintas al id de pregunta
df_1 = df.drop(df_1[df_1['pregunta_id'] != 15].index) #CLIENTES
df_2 = df.drop(df_2[df_2['pregunta_id'] != 16].index) #PROPUESTA DE VALOR
df_3 = df.drop(df_3[df_3['pregunta_id'] != 37].index) #CANALES
df_4 = df.drop(df_4[df_4['pregunta_id'] != 18].index) #RELACIÓN C/CLIENTES
df_5 = df.drop(df_5[df_5['pregunta_id'] != 19].index) #INGRESOS
df_6 = df.drop(df_6[df_6['pregunta_id'] != 20].index) #RECURSOS CLAVE
df_7 = df.drop(df_7[df_7['pregunta_id'] != 21].index) #ACTIVIDADES CLAVE
df_8 = df.drop(df_8[df_8['pregunta_id'] != 22].index) #COSTOS
df_9 = df.drop(df_9[df_9['pregunta_id'] != 23].index) #SOCIOS CLAVE

#contadores de respuestas x id
count_15 = (df['pregunta_id'] == 15).sum()
count_16 = (df['pregunta_id'] == 16).sum()
count_37 = (df['pregunta_id'] == 37).sum()
count_18 = (df['pregunta_id'] == 18).sum()
count_19 = (df['pregunta_id'] == 19).sum()
count_20 = (df['pregunta_id'] == 20).sum()
count_21 = (df['pregunta_id'] == 21).sum()
count_22 = (df['pregunta_id'] == 22).sum()
count_23 = (df['pregunta_id'] == 23).sum()
print(count_15, count_16, count_37, count_18, count_19, count_20, count_21, count_22, count_23)

#SEPARADOR DE DATAFRAMES CON Y SIN NOTA

df_nan1 = df_1[df_1['nota'].isna().copy()]
print(df_nan1)
df_CN1 = df_1[df_1['nota'].notna().copy()]
print(df_CN1)

df_nan2 = df_2[df_2['nota'].isna()]
print(df_nan2)
df_CN2 = df_2[df_2['nota'].notna()]
print(df_CN2)

df_nan3 = df_3[df_3['nota'].isna()]
print(df_nan3)
df_CN3 = df_3[df_3['nota'].notna()]
print(df_CN3)

df_nan4 = df_4[df_4['nota'].isna()]
print(df_nan4)
df_CN4 = df_4[df_4['nota'].notna()]
print(df_CN4)

df_nan5 = df_5[df_5['nota'].isna()]
print(df_nan5)
df_CN5 = df_5[df_5['nota'].notna()]
print(df_CN5)

df_nan6 = df_6[df_6['nota'].isna()]
print(df_nan6)
df_CN6 = df_6[df_6['nota'].notna()]
print(df_CN6)

df_nan7 = df_7[df_7['nota'].isna()]
print(df_nan7)
df_CN7 = df_7[df_7['nota'].notna()]
print(df_CN7)

df_nan8 = df_8[df_8['nota'].isna()]
print(df_nan8)
df_CN8 = df_8[df_8['nota'].notna()]
print(df_CN8)

df_nan9 = df_9[df_9['nota'].isna()]
print(df_nan9)
df_CN9 = df_9[df_9['nota'].notna()]
print(df_CN9)

#BORRA LAS FILAS CON NOTA 0
df_CN1 = df_CN1[df_CN1['nota'] != 0]
df_CN2 = df_CN2[df_CN2['nota'] != 0]
df_CN3 = df_CN3[df_CN3['nota'] != 0]
df_CN4 = df_CN4[df_CN4['nota'] != 0]
df_CN5 = df_CN5[df_CN5['nota'] != 0]
df_CN6 = df_CN6[df_CN6['nota'] != 0]
df_CN7 = df_CN7[df_CN7['nota'] != 0]
df_CN8 = df_CN8[df_CN8['nota'] != 0]
df_CN9 = df_CN9[df_CN9['nota'] != 0]

#ELIMINA COLUMNA 'NOTA'
df_nan1 = df_nan1.drop('nota', axis=1)
df_nan2 = df_nan2.drop('nota', axis=1)
df_nan3 = df_nan3.drop('nota', axis=1)
df_nan4 = df_nan4.drop('nota', axis=1)
df_nan5 = df_nan5.drop('nota', axis=1)
df_nan6 = df_nan6.drop('nota', axis=1)
df_nan7 = df_nan7.drop('nota', axis=1)
df_nan8 = df_nan8.drop('nota', axis=1)
df_nan9 = df_nan9.drop('nota', axis=1)

#CONTADOR DE FILAS
numero_de_filas_CN = [df_CN1.shape[0], df_CN2.shape[0], df_CN3.shape[0], df_CN4.shape[0], df_CN5.shape[0], df_CN6.shape[0], df_CN7.shape[0], df_CN8.shape[0], df_CN9.shape[0]]
print('numero de filas CON NOTA')
print(numero_de_filas_CN)
numero_de_filas_SN = [df_nan1.shape[0], df_nan2.shape[0], df_nan3.shape[0], df_nan4.shape[0], df_nan5.shape[0], df_nan6.shape[0], df_nan7.shape[0], df_nan8.shape[0], df_nan9.shape[0]]
print('numero de filas SIN NOTA')
print(numero_de_filas_SN)

df_nan1.head()

#CREACIÓN DE CSV PARA CADA DF

ClientesSN = 'Clientes_SN.csv'
df_nan1.to_csv(ClientesSN, index=False)
ClientesCN = 'Clientes_CN.csv'
df_CN1.to_csv(ClientesCN, index=False)

PValorSN = 'PValor_SN.csv'
df_nan2.to_csv(PValorSN, index=False)
PValorCN = 'PValor_CN.csv'
df_CN2.to_csv(PValorCN, index=False)

CanalesSN = 'Canales_SN.csv'
df_nan3.to_csv(CanalesSN, index=False)
CanalesCN = 'Canales_CN.csv'
df_CN3.to_csv(CanalesCN, index=False)

RelacionSN = 'Relacion_SN.csv'
df_nan4.to_csv(RelacionSN, index=False)
RelacionCN = 'Relacion_CN.csv'
df_CN4.to_csv(RelacionCN, index=False)

IngresosSN = 'Ingresos_SN.csv'
df_nan5.to_csv(IngresosSN, index=False)
IngresosCN = 'Ingresos_CN.csv'
df_CN5.to_csv(IngresosCN, index=False)

RecursosSN = 'Recursos_SN.csv'
df_nan6.to_csv(RecursosSN, index=False)
RecursosCN = 'Recursos_CN.csv'
df_CN6.to_csv(RecursosCN, index=False)

ActividadesSN = 'Actividades_SN.csv'
df_nan7.to_csv(ActividadesSN, index=False)
ActividadesCN = 'Actividades_CN.csv'
df_CN7.to_csv(ActividadesCN, index=False)

CostosSN = 'Costos_SN.csv'
df_nan8.to_csv(CostosSN, index=False)
CostosCN = 'Costos_CN.csv'
df_CN8.to_csv(CostosCN, index=False)

SociosSN = 'Socios_SN.csv'
df_nan9.to_csv(SociosSN, index=False)
SociosCN = 'Socios_CN.csv'
df_CN9.to_csv(SociosCN, index=False)

"""# **ESTADÍSTICAS DESCRIPTIVAS**"""

columnas_deseadas = ['texto.respuesta', 'nota']

df_CN1['nota'].describe()

df_CN1['nota'].unique()

"""# **LIBRERÍAS**"""

!pip install transformers
!pip install torch
!pip install optuna
!pip install nltk
!pip install stopwords

"""# **BETO**"""

#LLAMADA DE CSV
path = r"/content/Clientes_CN.csv"
df = pd.read_csv(path, sep=',')

from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
import torch
import optuna
from transformers import TrainerCallback
class MyOptunaCallback(TrainerCallback):
    def on_evaluate(self, args, state, control, metrics, **kwargs):
        control.should_training_stop = True
        return control

def model_init():
    return BertForSequenceClassification.from_pretrained("dccuchile/bert-base-spanish-wwm-uncased", num_labels=4)

from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')
    acc = accuracy_score(labels, predictions)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

"""HASTA ACÁ SE TRABAJARON LAS 9 SECCIONES DEL CANVAS
/ HACIA ABAJO SOLO SE ESTÁ VIENDO ''CLIENTES''
"""

#DISTRIBUCION DE PALABRAS x CATEGORIA
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
import matplotlib.pyplot as plt
import numpy as np

#df=df_CN1 #df auxiliar
df = df[df['nota'] != 0] #BORRA LAS FILAS CON NOTA 0
df['texto.respuesta'] = df['texto.respuesta'].astype(str)

df['word_count'] = df['texto.respuesta'].apply(lambda x: len(word_tokenize(x)))

stats_df = df.groupby('nota')['word_count'].agg(['mean', 'median', 'min', 'max', 'std'])
print(stats_df)

mode_df = df.groupby('nota')['word_count'].apply(lambda x: x.mode()[0]).reset_index()
mode_df.columns = ['nota', 'mode']
print(mode_df)

df.boxplot(column='word_count', by='nota')
plt.title('Distribución de la cantidad de palabras por cada categoría')
plt.suptitle('') # Eliminar el título automático
plt.xlabel('nota')  # Título del eje X
plt.ylabel('Cantidad de palabras')  # Título del eje Y
plt.show()

#ELIMINACION DE PALABRAS
import nltk
nltk.download('stopwords')
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords

df['texto.respuesta'] = df['texto.respuesta'].astype(str)

stop_words = set(stopwords.words('spanish'))

def count_words_and_stopwords(texto_respuesta):
    words = word_tokenize(texto_respuesta)
    word_count = len([word for word in words if word.casefold() not in stop_words])
    stopwords_in_texto_respuesta = [word for word in words if word.casefold() in stop_words]
    return word_count, stopwords_in_texto_respuesta

df['word_count'], df['stopwords_in_texto_respuesta'] = zip(*df['texto.respuesta'].apply(count_words_and_stopwords))

stats_df = df.groupby('nota')['word_count'].agg(['mean', 'median', 'min', 'max', 'std'])

mode_df = df.groupby('nota')['word_count'].apply(lambda x: x.mode()[0]).reset_index()
mode_df.columns = ['nota', 'mode']

print("Stopwords eliminadas: ", df['stopwords_in_texto_respuesta'].explode().unique())

import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords

df['texto.respuesta'] = df['texto.respuesta'].astype(str)

stop_words = set(stopwords.words('spanish'))

df['word_count'] = df['texto.respuesta'].apply(lambda x: len([word for word in word_tokenize(x) if word.casefold() not in stop_words]))

stats_df = df.groupby('nota')['word_count'].agg(['mean', 'median', 'min', 'max', 'std'])
print(stats_df)

mode_df = df.groupby('nota')['word_count'].apply(lambda x: x.mode()[0]).reset_index()
mode_df.columns = ['nota', 'mode']
print(mode_df)

df.boxplot(column='word_count', by='nota')
plt.title('Distribución de la cantidad de palabras por cada categoría')
plt.suptitle('')
plt.xlabel('nota')
plt.ylabel('Cantidad de palabras')
plt.show()

# Definir una función para extraer las palabras clave
def extract_keywords(text, num_keywords):
    vectorizer = TfidfVectorizer(stop_words=stopwords.words('spanish'))
    tfidf_matrix = vectorizer.fit_transform(text)
    words = vectorizer.get_feature_names_out()
    sums = tfidf_matrix.sum(axis=0)
    data = []
    for col, word in enumerate(words):
        data.append( (word, sums[0,col] ))
    ranking = pd.DataFrame(data, columns=['word','rank'])
    words_df = ranking.sort_values('rank', ascending=False)
    return words_df['word'].head(num_keywords)

# Aplicar la función a cada categoría
df['texto.respuesta'] = df['texto.respuesta'].astype(str)
keywords_per_category = df.groupby('nota')['texto.respuesta'].apply(lambda texts: extract_keywords(texts, 10))
print(keywords_per_category)

"""# **Evaluador**"""

category_count = df['nota'].value_counts()
categories = category_count.index
df['encoded_text'] = df['nota'].astype('category').cat.codes

data_texts = df['texto.respuesta'].to_list()
data_labels = df['encoded_text'].to_list()

train_texts, val_texts, train_labels, val_labels = train_test_split(data_texts, data_labels, random_state = 1800)

# para testear dentro de muestra
# train_texts, test_texts, train_labels, test_labels = train_test_split(train_texts, train_labels, test_size = 0.02, random_state = 2000)

# Cuenta de cada categoría
category_count = df['nota'].value_counts()

# Imprime el conteo de cada categoría
print(category_count)

print("Conjunto de datos de entrenamiento:", len(train_texts))
print("Conjunto de datos de prueba:", len(val_texts))

count_0 = train_labels.count(0)
count_1 = train_labels.count(1)
count_2 = train_labels.count(2)
count_3 = train_labels.count(3)

print("")
print("Datos de entrenamiento:")
print("Nota 1:", count_0)
print("Nota 4:", count_1)
print("Nota 6:", count_2)
print("Nota 7:", count_3)

count_t_0 = val_labels.count(0)
count_t_1 = val_labels.count(1)
count_t_2 = val_labels.count(2)
count_t_3 = val_labels.count(3)
print("")
print("Datos de prueba:")
print("Nota 1:", count_t_0)
print("Nota 4:", count_t_1)
print("Nota 6:", count_t_2)
print("Nota 7:", count_t_3)

tokenizer = BertTokenizer.from_pretrained("dccuchile/bert-base-spanish-wwm-uncased")

train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors="pt")
val_encodings = tokenizer(val_texts, truncation=True, padding=True, return_tensors="pt")

train_labels = torch.tensor(train_labels)
val_labels = torch.tensor(val_labels)

from torch.utils.data import Dataset

class TextClassificationDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = TextClassificationDataset(train_encodings, train_labels)
val_dataset = TextClassificationDataset(val_encodings, val_labels)

model = BertForSequenceClassification.from_pretrained("dccuchile/bert-base-spanish-wwm-uncased", num_labels=4)

!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install huggingface-hub --upgrade
!pip install numpy --upgrade
!pip install packaging --upgrade
!pip install psutil --upgrade
!pip install pyyaml --upgrade
!pip install accelerate --upgrade

import numpy as np
import time
import optuna
from transformers import TrainingArguments
from transformers import Trainer

def objective(trial):
    learning_rate = trial.suggest_float("learning_rate", 1e-6, 1e-4, log=True)
    num_train_epochs = trial.suggest_int("num_train_epochs", 1,5)
    per_device_train_batch_size = trial.suggest_categorical("per_device_train_batch_size", [8, 16, 32, 64])

# Otros parámetros:
#     seed = trial.suggest_int("seed", 1, 40)
#     weight_decay = trial.suggest_float("weight_decay", 0.0, 0.3)
#     warmup_steps = trial.suggest_int("warmup_steps", 0, 1000)
#     eval_steps = trial.suggest_int("eval_steps", 10, 100)
#     per_device_eval_batch_size = trial.suggest_categorical("per_device_eval_batch_size", [8, 16, 32, 64])

    args = TrainingArguments(
        output_dir="results",
        learning_rate=learning_rate,
        num_train_epochs=num_train_epochs,
        per_device_train_batch_size=per_device_train_batch_size,
        logging_dir="logs",
        evaluation_strategy="epoch",
    )

# seed=seed,
# per_device_eval_batch_size=64,   # batch size for evaluation
# weight_decay=weight_decay,
# warmup_steps=warmup_steps,
# eval_steps= eval_steps,
# per_device_eval_batch_size=per_device_eval_batch_size,

    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics
    )

    trainer.train()
    eval_metrics = trainer.evaluate()
    print(eval_metrics)

    return eval_metrics["eval_loss"]

start = time.time()

study = optuna.create_study(direction="minimize")

study.optimize(objective, n_trials=10)
print("Tiempo de ejecución:", time.time() - start, "segundos")